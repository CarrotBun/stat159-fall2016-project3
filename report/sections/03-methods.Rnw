---
output: pdf_document
---
## Methods  

###Standardization  
The colleges information data contains data measured in different scales such as admissions rate versus age (which are on very different scales). To prevent any biased weighting, we want to standardize the information before modeling.  

First, we created inidicator variables for the qualitative variables discussed in the introduction. Next, we want to standardize the absolute quantitites to relative quantities (mean centering).    

One reason to standardize variables is to have comparable scales. When you perform a regression analysis, the value of the computed coefficients will depend on the measurement scale of the associated predictors.  

In addition to standardizing, we removed all observations that have NA's.


###Regression Models (Technical Explanation)  

####*Ordinary Least Squares Regression (OLS)*  
If predictors (regressors) are correlated, the stability of the $\hat{beta}$ decreases, meaning, every estimate of $\beta$ could be very different and not converge to the true population coefficient. 

$$Balance = \beta_1 Married + \beta_2 Income + \beta_3 1st Generation + ...$$  

####*Ridge Regression (RR)*  
RR is a variation of the minimization in OLS Regression but with a constraint of $||\beta||^2_2 < c^2$.  

In vector form: $min \beta$ $||y-A\beta||^2_2 + \lambda||\beta||^2_2$ 

A difference in behavior of RR is that as $\lambda$ increases, more weight is given to the second term in the minimization. This means that with a large $\lambda$, the $\beta$ will be small.  

The main advantage of RR is that it takes correlated parameters into account and does automatics parameter weighing. 

####*Lasso Regression (LR)*  
LR is a variation of the minimization in OLS Regression but with a constraint of $||\beta||_1 < c$. With c, the constraint shape becomes a diamond and any pairs of $\beta$ will likely contain zeros. Unlike RR, there is no explicit form of $\beta$.

In vector form: $min \beta$ $||y-A\beta||^2_2 + \lambda||\beta||_1$  

The main advantage of LR is that it performs both parameter shrinkage through feature selection (sparsify regressors/predictors) and variable selection automatically.  

####*Principal Component Regression (PCR)*
PCR is based on principal component analysis. The goal of this method is to reduce the dimensions (created by the set of data points in n-dimensional space).  

To do so, we want define a direction, the first principal component, that maximizes the the variability in the data set and set the second principal component perpendicular to this first principal component. As a result, each data point's coordinates will change to this new coordinate system.  

####*Partial Least Squares Regression (PLSR)*
PLSR, similar to PCR, is also a dimensionality reduction method. While PCR finds hyperplanes of maximum variance between the predictors and responses, PLSR projects the predicted variables and the observable variables into a new space.   

The main advantage is that PLSR uses the annotated label to maximize inter-class variance. It takes into account of the classes and tries to reduce the dimension while maximizing the separation of classes.  


###Cross Validation and Train-Test Sets
Because we have limited amount of observations to build and test the model and we want to prevent bias, we will build and test the model using different subsets of the whole data set.  

We built train sets using 75% of the observations and test sets of the remaining 25% by random sampling (without replacement). We repeated this process 10 times for a 10 fold cross validation when we ran the regressions.  
  
  
